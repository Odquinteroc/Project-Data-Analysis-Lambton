{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerias \n",
    "#redes neuronales\n",
    "import numpy as np\n",
    "np.random.seed(4)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "#modelo arima\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "#regresion lineal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#importar documentos\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pymongo\n",
    "import os\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#ignorar advertencias python\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#medir tiemop\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importacion documentos\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "# Retrieve the MongoDB connection URI from the environment variables\n",
    "dbURI = os.getenv(\"MONGODB_URI\")\n",
    "# Connect to MongoDB using the connection URI\n",
    "client = pymongo.MongoClient(dbURI)\n",
    "# Access the \"Project\" database (it's not physically created until it has content)\n",
    "db = client['Project']\n",
    "#creacion datos capacidad\n",
    "collection = db['capacity']\n",
    "documents = collection.find()\n",
    "\n",
    "data = []\n",
    "for doc in documents:\n",
    "    province = doc[\"_id\"]\n",
    "    for energy_type, year_data in doc.items():\n",
    "        if energy_type != \"_id\":  # Skip the _id field\n",
    "            for year, value in year_data.items():\n",
    "                data.append({\n",
    "                    \"province\": province,\n",
    "                    \"energy type\": energy_type,\n",
    "                    \"year\": int(year),\n",
    "                    \"value\": value\n",
    "                })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "#print(df)\n",
    "\n",
    "#creacion base de datos generacion\n",
    "collection1 = db['generation']\n",
    "documents1 = collection1.find()\n",
    "\n",
    "data1 = []\n",
    "for doc in documents1:\n",
    "    province = doc[\"_id\"]\n",
    "    for energy_type, year_data in doc.items():\n",
    "        if energy_type != \"_id\":  # Skip the _id field\n",
    "            for year, value in year_data.items():\n",
    "                data1.append({\n",
    "                    \"province\": province,\n",
    "                    \"energy type\": energy_type,\n",
    "                    \"year\": int(year),\n",
    "                    \"value\": value\n",
    "                })\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "#print(df1)\n",
    "\n",
    "#creacion base de datos capacidad por sector \n",
    "collection2 = db['capacity_by_sector']\n",
    "documents2 = collection2.find()\n",
    "\n",
    "data2 = []\n",
    "\n",
    "for document in documents2:\n",
    "    sector = document[\"_id\"]\n",
    "    # Iterate over each year in the document\n",
    "    for year in document:\n",
    "        if year != \"_id\" and isinstance(document[year], dict):  # Check if it's a year key and the value is a dictionary\n",
    "            # Access the subdocument for the year\n",
    "            year_data = document[year]\n",
    "            # Append a dictionary with the desired structure to the data list\n",
    "            data2.append({\n",
    "                \"sector\": sector,\n",
    "                \"year\": year,\n",
    "                \"UOM\": year_data[\"UOM\"],\n",
    "                \"value\": year_data[\"VALUE\"],\n",
    "                \"growth_percentage\": year_data.get(\"Grow_percentage\", None)  # Using .get() to handle missing data\n",
    "            })\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "##informacion de tabla precios\n",
    "collection3 = db['prices']\n",
    "documents3 = collection3.find()\n",
    "data3 = []\n",
    "\n",
    "for doc in documents3:\n",
    "    province = doc[\"_id\"]\n",
    "    # Iterate over each sector in the document\n",
    "    for sector, years in doc.items():\n",
    "        if sector != \"_id\":  \n",
    "            # Iterate over each year in the sector\n",
    "            for year, value in years.items():\n",
    "                data3.append({\n",
    "                    \"Province\": province,\n",
    "                    \"Sector\": sector,\n",
    "                    \"year\": int(year),\n",
    "                    \"value\": value\n",
    "                })\n",
    "\n",
    "df3 = pd.DataFrame(data3)\n",
    "\n",
    "#importacion tabla demanda\n",
    "collection4 = db['demand']\n",
    "documents4 = collection4.find()\n",
    "data4 = []\n",
    "\n",
    "for doc in documents4:\n",
    "    province = doc[\"_id\"]\n",
    "    # Iterate over each sector in the document\n",
    "    for sector, years in doc.items():\n",
    "        if sector != \"_id\":  \n",
    "            # Iterate over each year in the sector\n",
    "            for year, value in years.items():\n",
    "                data4.append({\n",
    "                    \"Province\": province,\n",
    "                    \"Sector\": sector,\n",
    "                    \"year\": int(year),\n",
    "                    \"value\": value\n",
    "                })\n",
    "\n",
    "df4 = pd.DataFrame(data4)\n",
    "\n",
    "#tabla de emisiones\n",
    "collection5 = db['emissions']\n",
    "documents5 = collection5.find()\n",
    "data5 = []\n",
    "\n",
    "for doc in documents5:\n",
    "    # The _id field here seems to represent a category rather than a province\n",
    "    category = doc[\"_id\"]\n",
    "    # Iterate over each year in the document\n",
    "    for year, value in doc.items():\n",
    "        if year != \"_id\":\n",
    "            data5.append({\n",
    "                \"Category\": category,\n",
    "                \"year\": int(year),\n",
    "                \"value\": value\n",
    "            })\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df5 = pd.DataFrame(data5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion para calcular rsme con dataframes\n",
    "def cal_RMSE(val_reales, val_predichos):\n",
    "    val_reales = val_reales.values\n",
    "    val_predichos = val_predichos.values\n",
    "\n",
    "    mse = mean_squared_error(val_reales, val_predichos)\n",
    "\n",
    "    # Calcula el RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion para calcular R2 con dataframes\n",
    "def cal_R2(val_reales, val_predichos):\n",
    "    # Asegurarse de que val_reales y val_predichos sean arrays de numpy\n",
    "    val_reales = val_reales.values\n",
    "    val_predichos = val_predichos.values\n",
    "\n",
    "    # Calcular R^2\n",
    "    r2 = r2_score(val_reales, val_predichos)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "#RED NEURONAL\n",
    "\n",
    "\n",
    "\n",
    "def Redneuronal(datas,periodos_validacion):\n",
    "    #Definicion parametros para ajustar ddatos entrada funcion\n",
    "    p = periodos_validacion\n",
    "    time_step = 3\n",
    "    largo_setvalidacion = p + time_step\n",
    "    largo_data = len(datas)\n",
    "    largo_setentrenamiento = largo_data - largo_setvalidacion\n",
    "    #Definicion set entrenamiento y validacion\n",
    "    set_entrenamiento = datas.iloc[:largo_setentrenamiento+1]\n",
    "    set_validacion = datas.iloc[largo_setentrenamiento:largo_data]\n",
    "\n",
    "    #Normalización del set de entrenamiento\n",
    "    sc = MinMaxScaler(feature_range=(0,1)) \n",
    "    set_entrenamiento_escalado = sc.fit_transform(set_entrenamiento) \n",
    "\n",
    "    #Definicion de parametros\n",
    "    #time_step = 3\n",
    "    X_train = [] \n",
    "    Y_train = []\n",
    "    m = len(set_entrenamiento_escalado) \n",
    "\n",
    "    #sets de entrenamiento\n",
    "    for i in range(time_step,m):\n",
    "        X_train.append(set_entrenamiento_escalado[i-time_step:i,0])\n",
    "        Y_train.append(set_entrenamiento_escalado[i,0])\n",
    "\n",
    "    #ajustes para el modelo keras\n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "    #Creacion red LSTM\n",
    "    dim_entrada = (X_train.shape[1],1)\n",
    "    dim_salida = 1\n",
    "    na = 500\n",
    "\n",
    "    modelo = Sequential()\n",
    "    modelo.add(LSTM(units=na, input_shape=dim_entrada))\n",
    "    modelo.add(Dense(units=dim_salida))\n",
    "\n",
    "    modelo.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "    modelo.fit(X_train,Y_train,epochs=200 ,batch_size=400, verbose=0)\n",
    "\n",
    "    #Validación (predicción del valor de la energia)\n",
    "    x_test = set_validacion.values\n",
    "    x_test = sc.transform(x_test)\n",
    "    X_test = []\n",
    "    for i in range(time_step,len(x_test)):\n",
    "        X_test.append(x_test[i-time_step:i,0])\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "\n",
    "    if len(X_test) > 0:\n",
    "        prediccion = modelo.predict(X_test)\n",
    "        prediccion = sc.inverse_transform(prediccion)\n",
    "    else:\n",
    "        print(\"No hay suficientes datos para generar predicciones.\")\n",
    "\n",
    "    #ajustes del largo\n",
    "    prediction_length = len(prediccion)\n",
    "    validation_length = len(set_validacion)\n",
    "    new_length = validation_length - prediction_length\n",
    "\n",
    "    adjusted_validation_set = set_validacion.iloc[new_length:].copy()\n",
    "    index_names = adjusted_validation_set.index\n",
    "\n",
    "    # Convert prediction to DataFrame and adjust the column name\n",
    "    prediction_df = pd.DataFrame(prediccion)\n",
    "    \n",
    "    #print(result)\n",
    "\n",
    "    #sacar RSME\n",
    "    val_RSME = cal_RMSE(adjusted_validation_set,prediction_df)\n",
    "\n",
    "  \n",
    "    return val_RSME , cal_R2(adjusted_validation_set,prediction_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Modelo Arima\n",
    "def ModeloArima(datas,periodos_validacion):\n",
    "    #arreglando dataset largo\n",
    "    set_entrenamiento = datas.head(len(datas)-periodos_validacion)\n",
    "    set_validacion = datas.tail(periodos_validacion)\n",
    "    #entrenamiento modelo arima\n",
    "    modelo = auto_arima(\n",
    "        set_entrenamiento,\n",
    "        start_p=1, start_q=1, max_p=12, max_q=12, d=None, # Especifica rangos para p, d, q\n",
    "         # Prueba 'adf' para encontrar el orden de diferenciación óptimo d, si no se especifica\n",
    "        seasonal=False,  # Asume estacionalidad con un ciclo anual, ajusta según tus datos\n",
    "        stepwise=True,\n",
    "        test='kpss',\n",
    "        suppress_warnings=True,\n",
    "        error_action=\"ignore\",\n",
    "        n_jobs=-1 # Utiliza todos los núcleos disponibles para la búsqueda\n",
    "    )\n",
    "    # Hacer un pronóstico para los próximos  años \n",
    "    pronostico = modelo.predict(n_periods=periodos_validacion)\n",
    " \n",
    "\n",
    "    #sacar RSME\n",
    "    val_RSME = cal_RMSE(set_validacion, pronostico)\n",
    "\n",
    "    return  val_RSME, cal_R2(set_validacion, pronostico)\n",
    "\n",
    "# ModeloArima(datafiltrada,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RegresionLineal(datas,periodos_validacion):\n",
    "    #df3 = pd.DataFrame()\n",
    "\n",
    "    X = datas.index.values.reshape(-1, 1)    #print(X)\n",
    "    Y = datas.values  # Variable dependiente\n",
    "\n",
    "    X_train = X[:-periodos_validacion]\n",
    "    X_test = X[-periodos_validacion:]\n",
    "    y_train = Y[:-periodos_validacion]\n",
    "    y_test = Y[-periodos_validacion:]\n",
    "\n",
    "    # Entrenamiento del modelo de regresión lineal\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predicción para los años en el conjunto de prueba\n",
    "    y_predict = model.predict(X_test)\n",
    "    y_predict =pd.DataFrame(y_predict)\n",
    "    #sacar RSME\n",
    "    set_validacion = datas[-periodos_validacion:]\n",
    "    val_RSME = cal_RMSE(set_validacion, y_predict)\n",
    "\n",
    "    return  val_RSME , cal_R2(set_validacion, y_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_errores_optimizado(data, iterables2D, periodos):\n",
    "    resultados = []\n",
    "    #data['year'] = data['year'].astype(int)\n",
    "    #data['year'] = pd.to_datetime(data['year'], format='%Y')\n",
    "    \n",
    "    for i in data[iterables2D[0]].unique():\n",
    "        \n",
    "        for j in data[iterables2D[1]].unique():\n",
    "            # Filtra el DataFrame según las condiciones dadas\n",
    "            data_filtrada = data[(data[iterables2D[0]] == i) & (data[iterables2D[1]] == j)]\n",
    "\n",
    "            # Asegúrate de que la filtración no resulte en un DataFrame vacío\n",
    "            if not data_filtrada.empty:\n",
    "                # Selecciona solo las columnas de interés\n",
    "                datafiltrada = data_filtrada[[\"year\",\"value\"]]\n",
    "                datafiltrada = datafiltrada.set_index(\"year\")\n",
    "                datafiltrada.index.name = None\n",
    "    \n",
    "                # Llama a cada uno de los modelos y recopila sus resultados\n",
    "                RSME_ARI, R2_ARI = ModeloArima(datafiltrada, periodos)\n",
    "                RSME_RED, R2_RED = Redneuronal(datafiltrada, periodos)\n",
    "                RSME_RL, R2_RL = RegresionLineal(datafiltrada, periodos)\n",
    "                \n",
    "                # Crea un diccionario con todos los resultados y las etiquetas correspondientes\n",
    "                resultado_temporal = {\n",
    "                    iterables2D[0]: i, \n",
    "                    iterables2D[1]: j, \n",
    "                    'RSME_ARI': RSME_ARI, \n",
    "                    'R2_ARI': R2_ARI,\n",
    "                    'RSME_RED':RSME_RED,\n",
    "                    'R2_RED':R2_RED,\n",
    "                    'RSME_RL':RSME_RL,\n",
    "                    'R2_RL':R2_RL,\n",
    "                }\n",
    "                # Añade el diccionario a la lista de resultados\n",
    "                resultados.append(resultado_temporal)\n",
    "\n",
    "                \n",
    "\n",
    "    # Devuelve la lista de resultados\n",
    "    return resultados\n",
    "\n",
    "\n",
    "\n",
    "#-----------------\n",
    "def calcular_errores_optimizadouni(data, iterables2D, periodos):\n",
    "    resultados = []\n",
    "    #data['year'] = data['year'].astype(int)\n",
    "    #data['year'] = pd.to_datetime(data['year'], format='%Y')\n",
    "    \n",
    "    for i in data[iterables2D[0]].unique():\n",
    "        \n",
    "        data_filtrada = data[(data[iterables2D[0]] == i) ]\n",
    "\n",
    "        # Asegúrate de que la filtración no resulte en un DataFrame vacío\n",
    "        if not data_filtrada.empty:\n",
    "            # Selecciona solo las columnas de interés\n",
    "            datafiltrada = data_filtrada[[\"year\",\"value\"]]\n",
    "            datafiltrada = datafiltrada.set_index(\"year\")\n",
    "            datafiltrada.index.name = None\n",
    "\n",
    "            # Llama a cada uno de los modelos y recopila sus resultados\n",
    "            RSME_ARI, R2_ARI = ModeloArima(datafiltrada, periodos)\n",
    "            RSME_RED, R2_RED = Redneuronal(datafiltrada, periodos)\n",
    "            RSME_RL, R2_RL = RegresionLineal(datafiltrada, periodos)\n",
    "            \n",
    "            # Crea un diccionario con todos los resultados y las etiquetas correspondientes\n",
    "            resultado_temporal = {\n",
    "                iterables2D[0]: i,  \n",
    "                'RSME_ARI': RSME_ARI, \n",
    "                'R2_ARI': R2_ARI,\n",
    "                'RSME_RED':RSME_RED,\n",
    "                'R2_RED':R2_RED,\n",
    "                'RSME_RL':RSME_RL,\n",
    "                'R2_RL':R2_RL,\n",
    "            }\n",
    "            # Añade el diccionario a la lista de resultados\n",
    "            resultados.append(resultado_temporal)\n",
    "\n",
    "            \n",
    "\n",
    "    # Devuelve la lista de resultados\n",
    "    return resultados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_capacity = calcular_errores_optimizado(df,[\"province\",\"energy type\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_generation = calcular_errores_optimizado(df1,[\"province\",\"energy type\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_prices = calcular_errores_optimizado(df3,[\"Province\",\"Sector\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_demand = calcular_errores_optimizado(df4,[\"Province\",\"Sector\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_emissions = calcular_errores_optimizadouni(df5,[\"Category\"], 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_capacity_by_sector = calcular_errores_optimizadouni(df2,[\"sector\"], 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_emissions\n",
    "# df_emissions = pd.DataFrame(df_emissions)\n",
    "# print(df_emissions)\n",
    "# #para emisiones redes neuronales\n",
    "# df_generation = pd.DataFrame(df_generation)\n",
    "# print(df_generation)\n",
    "# #para generacion redes neuronales\n",
    "# df_capacity_by_sector = pd.DataFrame(df_capacity_by_sector)\n",
    "# df_capacity_by_sector.groupby(\"sector\").sum()\n",
    "# df_capacity_by_sector\n",
    "# #para capacidad por redes neuronales\n",
    "# df_capacity\n",
    "# df_capacity = pd.DataFrame(df_capacity)\n",
    "# print(df_capacity)\n",
    "# #para capacidad redes neuronales\n",
    "# df_prices \n",
    "# df_prices = pd.DataFrame(df_prices)\n",
    "# print(df_prices.sum())\n",
    "# #para precios es arima\n",
    "# df_demand \n",
    "# df_demand = pd.DataFrame(df_demand)\n",
    "# print(df_demand.sum())\n",
    "# #para demanda es arima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--------------------------------------\n",
    "# creacion nueva red nuronal\n",
    "\n",
    "def RedNeuronalValFuturos(datas,periodos_a_predecir,time_step):\n",
    "\n",
    "    # Normalización de todo el conjunto de datos 'datas'\n",
    "    sc = MinMaxScaler(feature_range=(0, 1))\n",
    "    datas_escalado = sc.fit_transform(datas)\n",
    "\n",
    "    # Preparación de los datos de entrenamiento\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(time_step, len(datas_escalado)):\n",
    "        X_train.append(datas_escalado[i-time_step:i, 0])\n",
    "        Y_train.append(datas_escalado[i, 0])\n",
    "\n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "    # Definición y entrenamiento de la red LSTM\n",
    "    modelo = Sequential([\n",
    "        LSTM(500, input_shape=(time_step, 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    modelo.compile(optimizer='rmsprop', loss='mse')\n",
    "    modelo.fit(X_train, Y_train, epochs=200, batch_size=400, verbose=0)\n",
    "\n",
    "    # Preparación para predicciones futuras\n",
    "    # Aquí asumimos que quieres generar predicciones continuando desde el final de 'datas'\n",
    "    ultimo_input = datas_escalado[-time_step:]\n",
    "    X_pred = np.array([ultimo_input])\n",
    "    X_pred = np.reshape(X_pred, (X_pred.shape[0], X_pred.shape[1], 1))\n",
    "\n",
    "    predicciones_futuras = []\n",
    "    for _ in range(periodos_a_predecir):\n",
    "        pred_nueva = modelo.predict(X_pred)[0]\n",
    "        predicciones_futuras.append(pred_nueva)\n",
    "        \n",
    "        # Actualiza 'X_pred' para incluir la nueva predicción al final y quitar el valor más antiguo al inicio\n",
    "        X_pred = np.append(X_pred, [[pred_nueva]], axis=1)\n",
    "        X_pred = X_pred[:, 1:, :]\n",
    "\n",
    "    # Inversión de la normalización para obtener valores reales\n",
    "    predicciones_futuras = sc.inverse_transform(predicciones_futuras)\n",
    "\n",
    "    # Conversión de las predicciones a un formato más legible/fácil de manejar, como un DataFrame\n",
    "    predicciones_df = pd.DataFrame(predicciones_futuras, columns=['Predicción'])\n",
    "\n",
    "    return predicciones_df\n",
    "\n",
    "\n",
    "#----------------\n",
    "#aplicacion modelo por tabla\n",
    "def calcular_pronostico(data, iterables2D,periodos_a_predecir,time_step):\n",
    "    resultados = []\n",
    "\n",
    "    for i in data[iterables2D[0]].unique():\n",
    "        \n",
    "        for j in data[iterables2D[1]].unique():\n",
    "            # Filtra el DataFrame según las condiciones dadas\n",
    "            data_filtrada = data[(data[iterables2D[0]] == i) & (data[iterables2D[1]] == j)]\n",
    "\n",
    "            # Asegúrate de que la filtración no resulte en un DataFrame vacío\n",
    "            if not data_filtrada.empty:\n",
    "                # Selecciona solo las columnas de interés\n",
    "                datafiltrada = data_filtrada[[\"year\",\"value\"]]\n",
    "                datafiltrada = datafiltrada.set_index(\"year\")\n",
    "                datafiltrada.index.name = None\n",
    "            \n",
    "            #configurar los index \n",
    "            resultado_temporal = {\n",
    "                iterables2D[0]: i, \n",
    "                iterables2D[1]: j, \n",
    "                'Pronostico': predicciones_df(datafiltrada,periodos_a_predecir,time_step)\n",
    "            }\n",
    "\n",
    "\n",
    "    # Devuelve la lista de resultados\n",
    "    return resultados\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=calcular_pronostico(df,[\"province\",\"energy type\"],5,5)\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
