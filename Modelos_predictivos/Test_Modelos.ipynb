{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerias \n",
    "#redes neuronales\n",
    "import numpy as np\n",
    "np.random.seed(4)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "#modelo arima\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "#regresion lineal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#importar documentos\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pymongo\n",
    "import os\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#ignorar advertencias python\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importacion documentos\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "# Retrieve the MongoDB connection URI from the environment variables\n",
    "dbURI = os.getenv(\"MONGODB_URI\")\n",
    "# Connect to MongoDB using the connection URI\n",
    "client = pymongo.MongoClient(dbURI)\n",
    "# Access the \"Project\" database (it's not physically created until it has content)\n",
    "db = client['Project']\n",
    "#creacion datos capacidad\n",
    "collection = db['capacity']\n",
    "documents = collection.find()\n",
    "\n",
    "data = []\n",
    "for doc in documents:\n",
    "    province = doc[\"_id\"]\n",
    "    for energy_type, year_data in doc.items():\n",
    "        if energy_type != \"_id\":  # Skip the _id field\n",
    "            for year, value in year_data.items():\n",
    "                data.append({\n",
    "                    \"province\": province,\n",
    "                    \"energy type\": energy_type,\n",
    "                    \"year\": int(year),\n",
    "                    \"value\": value\n",
    "                })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "#print(df)\n",
    "\n",
    "#creacion base de datos generacion\n",
    "collection1 = db['generation']\n",
    "documents1 = collection1.find()\n",
    "\n",
    "data1 = []\n",
    "for doc in documents1:\n",
    "    province = doc[\"_id\"]\n",
    "    for energy_type, year_data in doc.items():\n",
    "        if energy_type != \"_id\":  # Skip the _id field\n",
    "            for year, value in year_data.items():\n",
    "                data1.append({\n",
    "                    \"province\": province,\n",
    "                    \"energy type\": energy_type,\n",
    "                    \"year\": int(year),\n",
    "                    \"value\": value\n",
    "                })\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "#print(df1)\n",
    "\n",
    "#creacion base de datos capacidad por sector \n",
    "collection2 = db['capacity_by_sector']\n",
    "documents2 = collection2.find()\n",
    "\n",
    "data2 = []\n",
    "\n",
    "for document in documents2:\n",
    "    sector = document[\"_id\"]\n",
    "    # Iterate over each year in the document\n",
    "    for year in document:\n",
    "        if year != \"_id\" and isinstance(document[year], dict):  # Check if it's a year key and the value is a dictionary\n",
    "            # Access the subdocument for the year\n",
    "            year_data = document[year]\n",
    "            # Append a dictionary with the desired structure to the data list\n",
    "            data2.append({\n",
    "                \"sector\": sector,\n",
    "                \"year\": year,\n",
    "                \"UOM\": year_data[\"UOM\"],\n",
    "                \"value\": year_data[\"VALUE\"],\n",
    "                \"growth_percentage\": year_data.get(\"Grow_percentage\", None)  # Using .get() to handle missing data\n",
    "            })\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "##informacion de tabla precios\n",
    "collection3 = db['prices']\n",
    "documents3 = collection3.find()\n",
    "data3 = []\n",
    "\n",
    "for doc in documents3:\n",
    "    province = doc[\"_id\"]\n",
    "    # Iterate over each sector in the document\n",
    "    for sector, years in doc.items():\n",
    "        if sector != \"_id\":  \n",
    "            # Iterate over each year in the sector\n",
    "            for year, value in years.items():\n",
    "                data3.append({\n",
    "                    \"Province\": province,\n",
    "                    \"Sector\": sector,\n",
    "                    \"year\": int(year),\n",
    "                    \"value\": value\n",
    "                })\n",
    "\n",
    "df3 = pd.DataFrame(data3)\n",
    "\n",
    "#importacion tabla demanda\n",
    "collection4 = db['demand']\n",
    "documents4 = collection4.find()\n",
    "data4 = []\n",
    "\n",
    "for doc in documents4:\n",
    "    province = doc[\"_id\"]\n",
    "    # Iterate over each sector in the document\n",
    "    for sector, years in doc.items():\n",
    "        if sector != \"_id\":  \n",
    "            # Iterate over each year in the sector\n",
    "            for year, value in years.items():\n",
    "                data4.append({\n",
    "                    \"Province\": province,\n",
    "                    \"Sector\": sector,\n",
    "                    \"year\": int(year),\n",
    "                    \"value\": value\n",
    "                })\n",
    "\n",
    "df4 = pd.DataFrame(data4)\n",
    "\n",
    "#tabla de emisiones\n",
    "collection5 = db['emissions']\n",
    "documents5 = collection5.find()\n",
    "data5 = []\n",
    "\n",
    "for doc in documents5:\n",
    "    # The _id field here seems to represent a category rather than a province\n",
    "    category = doc[\"_id\"]\n",
    "    # Iterate over each year in the document\n",
    "    for year, value in doc.items():\n",
    "        if year != \"_id\":\n",
    "            data5.append({\n",
    "                \"Category\": category,\n",
    "                \"year\": int(year),\n",
    "                \"value\": value\n",
    "            })\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df5 = pd.DataFrame(data5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablasXproXsect =[\"prices\", \"demand\"]\n",
    "Otrastablas = [\"capacity_by_sector\",\"emissions\",]\n",
    "\n",
    "#uniendo tablas por provincia y energia\n",
    "iterablesXproXener = [\"province\",\"energy type\"]\n",
    "dataframesXproXener = [df,df1]\n",
    "tablasXproXener = [\"capacity\",\"generation\",\"demand\"]\n",
    "for x, i in enumerate(dataframesXproXener):\n",
    "    i[\"tabla\"] = tablasXproXener[x]\n",
    "\n",
    "resultXproXsect=pd.concat(dataframesXproXener, ignore_index=True)\n",
    "#resultXproXsect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion para calcular rsme con dataframes\n",
    "def cal_RMSE(val_reales, val_predichos):\n",
    "    val_reales = val_reales.values\n",
    "    val_predichos = val_predichos.values\n",
    "\n",
    "    mse = mean_squared_error(val_reales, val_predichos)\n",
    "\n",
    "    # Calcula el RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion para calcular R2 con dataframes\n",
    "def cal_R2(val_reales, val_predichos):\n",
    "    # Asegurarse de que val_reales y val_predichos sean arrays de numpy\n",
    "    val_reales = val_reales.values\n",
    "    val_predichos = val_predichos.values\n",
    "\n",
    "    # Calcular R^2\n",
    "    r2 = r2_score(val_reales, val_predichos)\n",
    "\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "#RED NEURONAL\n",
    "\n",
    "\n",
    "def Redneuronal(datas,periodos_validacion):\n",
    "    #Definicion parametros para ajustar ddatos entrada funcion\n",
    "    p = periodos_validacion\n",
    "    time_step = 3\n",
    "    largo_setvalidacion = p + time_step\n",
    "    largo_data = len(datas)\n",
    "    largo_setentrenamiento = largo_data - largo_setvalidacion\n",
    "    #Definicion set entrenamiento y validacion\n",
    "    set_entrenamiento = datas.iloc[:largo_setentrenamiento+1]\n",
    "    set_validacion = datas.iloc[largo_setentrenamiento:largo_data]\n",
    "\n",
    "    #Normalización del set de entrenamiento\n",
    "    sc = MinMaxScaler(feature_range=(0,1)) \n",
    "    set_entrenamiento_escalado = sc.fit_transform(set_entrenamiento) \n",
    "\n",
    "    #Definicion de parametros\n",
    "    #time_step = 3\n",
    "    X_train = [] \n",
    "    Y_train = []\n",
    "    m = len(set_entrenamiento_escalado) \n",
    "\n",
    "    #sets de entrenamiento\n",
    "    for i in range(time_step,m):\n",
    "        X_train.append(set_entrenamiento_escalado[i-time_step:i,0])\n",
    "        Y_train.append(set_entrenamiento_escalado[i,0])\n",
    "\n",
    "    #ajustes para el modelo keras\n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "    #Creacion red LSTM\n",
    "    dim_entrada = (X_train.shape[1],1)\n",
    "    dim_salida = 1\n",
    "    na = 500\n",
    "\n",
    "    modelo = Sequential()\n",
    "    modelo.add(LSTM(units=na, input_shape=dim_entrada))\n",
    "    modelo.add(Dense(units=dim_salida))\n",
    "\n",
    "    modelo.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "    modelo.fit(X_train,Y_train,epochs=200,batch_size=400)\n",
    "\n",
    "    #Validación (predicción del valor de la energia)\n",
    "    x_test = set_validacion.values\n",
    "    x_test = sc.transform(x_test)\n",
    "    X_test = []\n",
    "    for i in range(time_step,len(x_test)):\n",
    "        X_test.append(x_test[i-time_step:i,0])\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "\n",
    "    if len(X_test) > 0:\n",
    "        prediccion = modelo.predict(X_test)\n",
    "        prediccion = sc.inverse_transform(prediccion)\n",
    "    else:\n",
    "        print(\"No hay suficientes datos para generar predicciones.\")\n",
    "\n",
    "    #ajustes del largo\n",
    "    prediction_length = len(prediccion)\n",
    "    validation_length = len(set_validacion)\n",
    "    new_length = validation_length - prediction_length\n",
    "\n",
    "    adjusted_validation_set = set_validacion.iloc[new_length:].copy()\n",
    "    index_names = adjusted_validation_set.index\n",
    "\n",
    "    # Convert prediction to DataFrame and adjust the column name\n",
    "    prediction_df = pd.DataFrame(prediccion, columns=[\"Wind_prediction\"])\n",
    "    result = pd.concat([adjusted_validation_set.reset_index(drop=True), prediction_df.reset_index(drop=True)], axis=1)\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "    result.index = index_names\n",
    "\n",
    "    #sacar RSME\n",
    "    val_RSME = cal_RMSE(result[\"Wind\"], result[\"Wind_prediction\"])\n",
    "\n",
    "  \n",
    "    return val_RSME , cal_R2(result[\"Wind\"], result[\"Wind_prediction\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------\n",
    "\n",
    "# datafiltrada = resultXproXsect[(resultXproXsect[\"province\"] == \"Alberta\") & (resultXproXsect[\"energy type\"] == \"Hydro / Wave / Tidal\")]\n",
    "# datafiltrada = datafiltrada[[\"year\",\"value\"]]\n",
    "# datafiltrada = datafiltrada.set_index(\"year\")\n",
    "# datafiltrada.index.name = None\n",
    "\n",
    "#se quito\n",
    "#test='kpss',\n",
    "\n",
    "#Modelo Arima\n",
    "def ModeloArima(datas,periodos_validacion):\n",
    "    #arreglando dataset largo\n",
    "    set_entrenamiento = datas.head(len(datas)-periodos_validacion)\n",
    "    set_validacion = datas.tail(periodos_validacion)\n",
    "    #entrenamiento modelo arima\n",
    "    modelo = auto_arima(\n",
    "        set_entrenamiento,\n",
    "        start_p=1, start_q=1, max_p=12, max_q=12, d=None, # Especifica rangos para p, d, q\n",
    "         # Prueba 'adf' para encontrar el orden de diferenciación óptimo d, si no se especifica\n",
    "        seasonal=True, m=12, # Asume estacionalidad con un ciclo anual, ajusta según tus datos\n",
    "        stepwise=True,\n",
    "        suppress_warnings=True,\n",
    "        error_action=\"ignore\",\n",
    "        n_jobs=-1 # Utiliza todos los núcleos disponibles para la búsqueda\n",
    "    )\n",
    "    # Hacer un pronóstico para los próximos  años \n",
    "    pronostico = modelo.predict(n_periods=periodos_validacion)\n",
    "    # Los años para los cuales se realiza el pronóstico\n",
    "    #años =  set_validacion.index.name\n",
    "\n",
    "    # Crear un DataFrame para mostrar los pronósticos de forma más amigable\n",
    "    #pronosticos_df = pd.DataFrame(data=pronostico, index=años, columns=['Pronóstico'])\n",
    "\n",
    "    #sacar RSME\n",
    "    val_RSME = cal_RMSE(set_validacion, pronostico)\n",
    "\n",
    "    return  val_RSME, cal_R2(set_validacion, pronostico)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegresionLineal(datas,periodos_validacion):\n",
    "    df3 = pd.DataFrame()\n",
    "\n",
    "    df3['Año'] = datas.index.year\n",
    "    X = df3['Año'].values.reshape(-1, 1) - 2000  # Variable independiente\n",
    "    y = datas['Wind'].values  # Variable dependiente\n",
    "\n",
    "    X_train_temp, X_test, y_train_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    # Entrenar el modelo de regresión lineal\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_temp, y_train_temp)\n",
    "\n",
    "    #anos a predecir\n",
    "    set_validacion = datas.tail(periodos_validacion)\n",
    "    #Predecir el modelo para los anos dichos\n",
    "    y_predict = model.predict(X[-periodos_validacion:])\n",
    "    años = set_validacion.index\n",
    "    pronosticos_df = pd.DataFrame(data=y_predict, index=años, columns=['Pronóstico'])\n",
    "    #print(pronosticos_df)\n",
    "    \n",
    "    #sacar RSME\n",
    "    val_RSME = cal_RMSE(set_validacion, pronosticos_df)\n",
    "\n",
    "    return  val_RSME , cal_R2(set_validacion, pronosticos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_errores_optimizado(data, iterables2D, periodos):\n",
    "    resultados = []\n",
    "    #data['year'] = data['year'].astype(int)\n",
    "    #data['year'] = pd.to_datetime(data['year'], format='%Y')\n",
    "    for i in data[iterables2D[0]].unique():\n",
    "        for j in data[iterables2D[1]].unique():\n",
    "            # Filtra el DataFrame según las condiciones dadas\n",
    "            data_filtrada = data[(data[iterables2D[0]] == i) & (data[iterables2D[1]] == j)]\n",
    "            print(i,j)\n",
    "            # Asegúrate de que la filtración no resulte en un DataFrame vacío\n",
    "            if not data_filtrada.empty:\n",
    "                # Selecciona solo las columnas de interés\n",
    "                datafiltrada = data_filtrada[[\"year\",\"value\"]]\n",
    "                datafiltrada = datafiltrada.set_index(\"year\")\n",
    "                datafiltrada.index.name = None\n",
    "                # Llama a cada uno de los modelos y recopila sus resultados\n",
    "                RSME_ARI, R2_ARI = ModeloArima(datafiltrada, periodos)\n",
    "                #RSME_RED, R2_RED = Redneuronal(datafiltrada, periodos)\n",
    "                #RSME_RL, R2_RL = RegresionLineal(datafiltrada, periodos)\n",
    "                \n",
    "                # Crea un diccionario con todos los resultados y las etiquetas correspondientes\n",
    "                resultado_temporal = {\n",
    "                    iterables2D[0]: i, \n",
    "                    iterables2D[1]: j, \n",
    "                    'RSME_ARI': RSME_ARI, \n",
    "                    'R2_ARI': R2_ARI,\n",
    "                  \n",
    "                }\n",
    "                # Añade el diccionario a la lista de resultados\n",
    "                resultados.append(resultado_temporal)\n",
    "\n",
    "    # Devuelve la lista de resultados\n",
    "    return resultados\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alberta Hydro / Wave / Tidal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alberta Wind\n",
      "Alberta Biomass / Geothermal\n",
      "Alberta Solar\n",
      "Alberta Uranium\n",
      "Alberta Coal & Coke\n",
      "Alberta Natural Gas\n",
      "Alberta Oil\n",
      "British Columbia Hydro / Wave / Tidal\n",
      "British Columbia Wind\n",
      "British Columbia Biomass / Geothermal\n",
      "British Columbia Solar\n",
      "British Columbia Uranium\n",
      "British Columbia Coal & Coke\n",
      "British Columbia Natural Gas\n",
      "British Columbia Oil\n",
      "Canada Hydro / Wave / Tidal\n",
      "Canada Wind\n",
      "Canada Biomass / Geothermal\n",
      "Canada Solar\n",
      "Canada Uranium\n",
      "Canada Coal & Coke\n",
      "Canada Natural Gas\n",
      "Canada Oil\n",
      "Manitoba Hydro / Wave / Tidal\n",
      "Manitoba Wind\n",
      "Manitoba Biomass / Geothermal\n",
      "Manitoba Solar\n",
      "Manitoba Uranium\n",
      "Manitoba Coal & Coke\n",
      "Manitoba Natural Gas\n",
      "Manitoba Oil\n",
      "New Brunswick Hydro / Wave / Tidal\n",
      "New Brunswick Wind\n",
      "New Brunswick Biomass / Geothermal\n",
      "New Brunswick Solar\n",
      "New Brunswick Uranium\n",
      "New Brunswick Coal & Coke\n",
      "New Brunswick Natural Gas\n",
      "New Brunswick Oil\n",
      "Newfoundland and Labrador Hydro / Wave / Tidal\n",
      "Newfoundland and Labrador Wind\n",
      "Newfoundland and Labrador Biomass / Geothermal\n",
      "Newfoundland and Labrador Solar\n",
      "Newfoundland and Labrador Uranium\n",
      "Newfoundland and Labrador Coal & Coke\n",
      "Newfoundland and Labrador Natural Gas\n",
      "Newfoundland and Labrador Oil\n",
      "Northwest Territories Hydro / Wave / Tidal\n",
      "Northwest Territories Wind\n",
      "Northwest Territories Biomass / Geothermal\n",
      "Northwest Territories Solar\n",
      "Northwest Territories Uranium\n",
      "Northwest Territories Coal & Coke\n",
      "Northwest Territories Natural Gas\n",
      "Northwest Territories Oil\n",
      "Nova Scotia Hydro / Wave / Tidal\n",
      "Nova Scotia Wind\n",
      "Nova Scotia Biomass / Geothermal\n",
      "Nova Scotia Solar\n",
      "Nova Scotia Uranium\n",
      "Nova Scotia Coal & Coke\n",
      "Nova Scotia Natural Gas\n",
      "Nova Scotia Oil\n",
      "Nunavut Hydro / Wave / Tidal\n",
      "Nunavut Wind\n",
      "Nunavut Biomass / Geothermal\n",
      "Nunavut Solar\n",
      "Nunavut Uranium\n",
      "Nunavut Coal & Coke\n",
      "Nunavut Natural Gas\n",
      "Nunavut Oil\n",
      "Ontario Hydro / Wave / Tidal\n",
      "Ontario Wind\n",
      "Ontario Biomass / Geothermal\n",
      "Ontario Solar\n",
      "Ontario Uranium\n",
      "Ontario Coal & Coke\n",
      "Ontario Natural Gas\n",
      "Ontario Oil\n",
      "Prince Edward Island Hydro / Wave / Tidal\n",
      "Prince Edward Island Wind\n",
      "Prince Edward Island Biomass / Geothermal\n",
      "Prince Edward Island Solar\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All lag values up to 'maxlag' produced singular matrices. Consider using a longer series, a different lag term or a different test.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mcalcular_errores_optimizado\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresultXproXsect\u001b[49m\u001b[43m,\u001b[49m\u001b[43miterablesXproXener\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m x\n",
      "Cell \u001b[1;32mIn[39], line 17\u001b[0m, in \u001b[0;36mcalcular_errores_optimizado\u001b[1;34m(data, iterables2D, periodos)\u001b[0m\n\u001b[0;32m     15\u001b[0m datafiltrada\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Llama a cada uno de los modelos y recopila sus resultados\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m RSME_ARI, R2_ARI \u001b[38;5;241m=\u001b[39m \u001b[43mModeloArima\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatafiltrada\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiodos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#RSME_RED, R2_RED = Redneuronal(datafiltrada, periodos)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#RSME_RL, R2_RL = RegresionLineal(datafiltrada, periodos)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Crea un diccionario con todos los resultados y las etiquetas correspondientes\u001b[39;00m\n\u001b[0;32m     22\u001b[0m resultado_temporal \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     23\u001b[0m     iterables2D[\u001b[38;5;241m0\u001b[39m]: i, \n\u001b[0;32m     24\u001b[0m     iterables2D[\u001b[38;5;241m1\u001b[39m]: j, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m   \n\u001b[0;32m     28\u001b[0m }\n",
      "Cell \u001b[1;32mIn[37], line 17\u001b[0m, in \u001b[0;36mModeloArima\u001b[1;34m(datas, periodos_validacion)\u001b[0m\n\u001b[0;32m     15\u001b[0m set_validacion \u001b[38;5;241m=\u001b[39m datas\u001b[38;5;241m.\u001b[39mtail(periodos_validacion)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#entrenamiento modelo arima\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m modelo \u001b[38;5;241m=\u001b[39m \u001b[43mauto_arima\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mset_entrenamiento\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Especifica rangos para p, d, q\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Prueba 'adf' para encontrar el orden de diferenciación óptimo d, si no se especifica\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseasonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Asume estacionalidad con un ciclo anual, ajusta según tus datos\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstepwise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuppress_warnings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Utiliza todos los núcleos disponibles para la búsqueda\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Hacer un pronóstico para los próximos  años \u001b[39;00m\n\u001b[0;32m     28\u001b[0m pronostico \u001b[38;5;241m=\u001b[39m modelo\u001b[38;5;241m.\u001b[39mpredict(n_periods\u001b[38;5;241m=\u001b[39mperiodos_validacion)\n",
      "File \u001b[1;32mc:\\Users\\ninic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pmdarima\\arima\\auto.py:506\u001b[0m, in \u001b[0;36mauto_arima\u001b[1;34m(y, X, start_p, d, start_q, max_p, max_d, max_q, start_P, D, start_Q, max_P, max_D, max_Q, max_order, m, seasonal, stationary, information_criterion, alpha, test, seasonal_test, stepwise, n_jobs, start_params, trend, method, maxiter, offset_test_args, seasonal_test_args, suppress_warnings, error_action, trace, random, random_state, n_fits, return_valid_fits, out_of_sample_size, scoring, scoring_args, with_intercept, sarimax_kwargs, **fit_args)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# m must be > 1 for nsdiffs\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# we don't have a D yet and we need one (seasonal)\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m     D \u001b[38;5;241m=\u001b[39m \u001b[43mnsdiffs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseasonal_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_D\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_D\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mseasonal_test_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m D \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m         diffxreg \u001b[38;5;241m=\u001b[39m diff(X, differences\u001b[38;5;241m=\u001b[39mD, lag\u001b[38;5;241m=\u001b[39mm)\n",
      "File \u001b[1;32mc:\\Users\\ninic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pmdarima\\arima\\utils.py:106\u001b[0m, in \u001b[0;36mnsdiffs\u001b[1;34m(x, m, max_D, test, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    105\u001b[0m D \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 106\u001b[0m dodiff \u001b[38;5;241m=\u001b[39m \u001b[43mtestfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m dodiff \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m D \u001b[38;5;241m<\u001b[39m max_D:\n\u001b[0;32m    108\u001b[0m     D \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ninic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pmdarima\\arima\\seasonality.py:597\u001b[0m, in \u001b[0;36mOCSBTest.estimate_seasonal_differencing_term\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    594\u001b[0m x \u001b[38;5;241m=\u001b[39m check_endog(x, dtype\u001b[38;5;241m=\u001b[39mDTYPE, preserve_series\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    596\u001b[0m \u001b[38;5;66;03m# Get the critical value for m\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m stat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_test_statistic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    598\u001b[0m crit_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_ocsb_crit_val(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(stat \u001b[38;5;241m>\u001b[39m crit_val)\n",
      "File \u001b[1;32mc:\\Users\\ninic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pmdarima\\arima\\seasonality.py:546\u001b[0m, in \u001b[0;36mOCSBTest._compute_test_statistic\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# If they're all NaN, raise\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(icvals)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll lag values up to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxlag\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m produced \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingular matrices. Consider using a longer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    548\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries, a different lag term or a different \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    549\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# Compute the information criterion vals\u001b[39;00m\n\u001b[0;32m    552\u001b[0m best_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mnanargmin(icvals))\n",
      "\u001b[1;31mValueError\u001b[0m: All lag values up to 'maxlag' produced singular matrices. Consider using a longer series, a different lag term or a different test."
     ]
    }
   ],
   "source": [
    "x = calcular_errores_optimizado(resultXproXsect,iterablesXproXener, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
