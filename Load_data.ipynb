{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pymongo\n",
    "import os\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the MongoDB connection URI from the environment variables\n",
    "dbURI = os.getenv(\"MONGODB_URI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB using the connection URI\n",
    "client = pymongo.MongoClient(dbURI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the databases in the MongoDB server\n",
    "databases = client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases in the MongoDB server:\n",
      "Mongo\n",
      "Project\n",
      "pymongo\n",
      "pymongodb\n",
      "sample_airbnb\n",
      "sample_analytics\n",
      "sample_geospatial\n",
      "sample_guides\n",
      "sample_mflix\n",
      "sample_restaurants\n",
      "sample_supplies\n",
      "sample_training\n",
      "sample_weatherdata\n",
      "admin\n",
      "local\n"
     ]
    }
   ],
   "source": [
    "# Print the list of database names\n",
    "print(\"Databases in the MongoDB server:\")\n",
    "for db_name in databases:\n",
    "    print(db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the \"Project\" database (it's not physically created until it has content)\n",
    "db = client['Project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly create the \"capacity\" collection\n",
    "db.create_collection('capacity')\n",
    "collection = db['capacity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly create the \"generation\" collection\n",
    "db.create_collection('generation')\n",
    "collection1 = db['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections in the 'Project' database after attempted creation:\n",
      "capacity_by_sector\n",
      "generation\n",
      "capacity\n"
     ]
    }
   ],
   "source": [
    "# List all collections (they won't appear in the list until they contain at least one document)\n",
    "print(\"Collections in the 'Project' database after attempted creation:\")\n",
    "for collection_name in db.list_collection_names():\n",
    "    print(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path folder CSV capacity files\n",
    "folder_path = 'capacity_csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult at 0x236df17e3b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all documents in the collection\n",
    "# collection2.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted for Alberta\n",
      "Data inserted for British Columbia\n",
      "Data inserted for Canada\n",
      "Data inserted for Manitoba\n",
      "Data inserted for New Brunswick\n",
      "Data inserted for Newfoundland and Labrador\n",
      "Data inserted for Northwest Territories\n",
      "Data inserted for Nova Scotia\n",
      "Data inserted for Nunavut\n",
      "Data inserted for Ontario\n",
      "Data inserted for Prince Edward Island\n",
      "Data inserted for Quebec\n",
      "Data inserted for Saskatchewan\n",
      "Data inserted for Yukon\n"
     ]
    }
   ],
   "source": [
    "# Process each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "        # Convert index to string if it's not\n",
    "        data.index = data.index.map(str)\n",
    "\n",
    "        # Fix the year columns by converting float to int, then to string if they are numbers\n",
    "        data.columns = [str(int(float(col))) if col.isdigit() or col.replace('.', '', 1).isdigit() else col for col in data.columns]\n",
    "\n",
    "        # Extract the province name from the file name\n",
    "        province_name = filename.replace('Capacity in ', '').split('.')[0]\n",
    "\n",
    "        # Initialize the document with the province name as _id\n",
    "        document = {\"_id\": province_name}\n",
    "\n",
    "        # Add each energy type with years as subdocuments\n",
    "        for energy_type, row in data.iterrows():\n",
    "            annual_data = {year: value for year, value in row.to_dict().items()}\n",
    "            document[energy_type] = annual_data\n",
    "\n",
    "        # Insert the data into MongoDB, replacing the existing document with the same _id\n",
    "        collection.replace_one({\"_id\": province_name}, document, upsert=True)\n",
    "        print(f\"Data inserted for {province_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path folder CSV capacity files\n",
    "folder_path1 = 'generation_csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted for Alberta\n",
      "Data inserted for British Columbia\n",
      "Data inserted for Canada\n",
      "Data inserted for Manitoba\n",
      "Data inserted for New Brunswick\n",
      "Data inserted for Newfoundland and Labrador\n",
      "Data inserted for Northwest Territories\n",
      "Data inserted for Nova Scotia\n",
      "Data inserted for Nunavut\n",
      "Data inserted for Ontario\n",
      "Data inserted for Prince Edward Island\n",
      "Data inserted for Quebec\n",
      "Data inserted for Saskatchewan\n",
      "Data inserted for Yukon\n"
     ]
    }
   ],
   "source": [
    "# Process each file in the folder\n",
    "for filename in os.listdir(folder_path1):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path1, filename)\n",
    "\n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "        # Convert index to string if it's not\n",
    "        data.index = data.index.map(str)\n",
    "\n",
    "        # Fix the year columns by converting float to int, then to string if they are numbers\n",
    "        data.columns = [str(int(float(col))) if col.isdigit() or col.replace('.', '', 1).isdigit() else col for col in data.columns]\n",
    "\n",
    "        # Extract the province name from the file name\n",
    "        province_name = filename.replace('Generation in ', '').split('.')[0]\n",
    "\n",
    "        # Initialize the document with the province name as _id\n",
    "        document = {\"_id\": province_name}\n",
    "\n",
    "        # Add each energy type with years as subdocuments\n",
    "        for energy_type, row in data.iterrows():\n",
    "            annual_data = {year: value for year, value in row.to_dict().items()}\n",
    "            document[energy_type] = annual_data\n",
    "\n",
    "        # Insert the data into MongoDB, replacing the existing document with the same _id\n",
    "        collection1.replace_one({\"_id\": province_name}, document, upsert=True)\n",
    "        print(f\"Data inserted for {province_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly create the \"capacity_by_sector\" collection\n",
    "db.create_collection('capacity_by_sector')\n",
    "collection2 = db['capacity_by_sector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path folder CSV capacity files\n",
    "folder_path2 = 'private_public_capacity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted for industries\n",
      "Data inserted for private electric utilities\n",
      "Data inserted for public electric utilities\n",
      "Data inserted for capacity in Total all classes of electricity producer\n"
     ]
    }
   ],
   "source": [
    "# Process each file in the folder\n",
    "for filename in os.listdir(folder_path2):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path2, filename)\n",
    "\n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "        # Convert index to string\n",
    "        data.index = data.index.map(str)\n",
    "\n",
    "        # Fix the year columns by converting float to int, then to string if they are numbers\n",
    "        #data.columns = [str(int(float(col))) if col.isdigit() or col.replace('.', '', 1).isdigit() else col for col in data.columns]\n",
    "\n",
    "        # Extract the sector name from the file name\n",
    "        sector_name = filename.replace('capacity in Electricity producer, ', '').split('.')[0]\n",
    "\n",
    "        # Initialize the document with the province name as _id\n",
    "        document = {\"_id\": sector_name}\n",
    "\n",
    "         # Iterate over each row in the DataFrame and construct the subdocuments\n",
    "        for index, row in data.iterrows():\n",
    "            year = str(row['REF_DATE'])\n",
    "            document[year] = {\n",
    "                \"UOM\": row['UOM'],\n",
    "                \"VALUE\": row['VALUE'],\n",
    "                \"Grow_percentage\": row['Porcentaje_Aumento']\n",
    "            }\n",
    "\n",
    "        # Insert the data into MongoDB, replacing the existing document with the same _id\n",
    "        collection2.replace_one({\"_id\": sector_name}, document, upsert=True)\n",
    "        print(f\"Data inserted for {sector_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
